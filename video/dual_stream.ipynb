{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Architecture of Dual-Stream networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, use_pool=True, pool_size=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.use_pool = use_pool\n",
    "        if use_pool:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.use_pool:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DualStreamNetwork(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Common stream structure (used for both spatial and temporal)\n",
    "        def create_stream():\n",
    "            return nn.Sequential(\n",
    "                # conv1: 7x7x96, stride 2, norm, pool 2x2\n",
    "                ConvBlock(3, 96, kernel_size=7, stride=2),\n",
    "                \n",
    "                # conv2: 5x5x256, stride 2, norm, pool 2x2\n",
    "                ConvBlock(96, 256, kernel_size=5, stride=2),\n",
    "                \n",
    "                # conv3: 3x3x512, stride 1\n",
    "                ConvBlock(256, 512, kernel_size=3, stride=1, use_pool=False),\n",
    "                \n",
    "                # conv4: 3x3x512, stride 1\n",
    "                ConvBlock(512, 512, kernel_size=3, stride=1, use_pool=False),\n",
    "                \n",
    "                # conv5: 3x3x512, stride 1, pool 2x2\n",
    "                ConvBlock(512, 512, kernel_size=3, stride=1)\n",
    "            )\n",
    "        \n",
    "        # Spatial stream\n",
    "        self.spatial_stream = create_stream()\n",
    "        \n",
    "        # Temporal stream (modify first conv layer for 2-channel input)\n",
    "        self.temporal_stream = create_stream()\n",
    "        # Replace first conv layer to accept 2-channel input\n",
    "        self.temporal_stream[0].conv = nn.Conv2d(2, 96, kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 2048)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.fc8 = nn.Linear(2048 * 2, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward_single_stream(self, x, stream):\n",
    "        # Convolutional layers\n",
    "        x = stream(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, spatial_input, temporal_input):\n",
    "        # Process streams\n",
    "        spatial_features = self.forward_single_stream(spatial_input, self.spatial_stream)\n",
    "        temporal_features = self.forward_single_stream(temporal_input, self.temporal_stream)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([spatial_features, temporal_features], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc8(combined_features)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_optical_flow(flow_path):\n",
    "    \"\"\"Load pre-computed optical flow data\"\"\"\n",
    "    try:\n",
    "        flow = np.load(flow_path)\n",
    "        # Ensure flow has correct dimensions (H, W, 2)\n",
    "        if flow.shape[-1] != 2:\n",
    "            raise ValueError(f\"Invalid flow shape: {flow.shape}\")\n",
    "        # Normalize flow values\n",
    "        flow = (flow - flow.min()) / (flow.max() - flow.min() + 1e-6)\n",
    "        flow = torch.from_numpy(flow.transpose(2, 0, 1)).float()\n",
    "        return flow\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading optical flow from {flow_path}: {e}\")\n",
    "        # Return zero tensor as fallback\n",
    "        return torch.zeros(2, 224, 224)\n",
    "\n",
    "def load_video_frame(frame_path):\n",
    "    \"\"\"Load and preprocess video frame\"\"\"\n",
    "    try:\n",
    "        frame = Image.open(frame_path)\n",
    "        frame = frame.convert('RGB')\n",
    "        frame = frame.resize((224, 224))\n",
    "        frame = np.array(frame) / 255.0\n",
    "        frame = torch.from_numpy(frame.transpose(2, 0, 1)).float()\n",
    "        return frame\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading frame from {frame_path}: {e}\")\n",
    "        # Return zero tensor as fallback\n",
    "        return torch.zeros(3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load dataset splits\n",
    "        self.frames_dir = os.path.join(root_dir, 'frames')\n",
    "        self.flows_dir = os.path.join(root_dir, 'flows')\n",
    "        \n",
    "        # Load video paths and labels\n",
    "        self.samples = []  # [(frame_path, flow_path, label), ...]\n",
    "        self._load_dataset()\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if split == 'train':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "    def _load_dataset(self):\n",
    "        \"\"\"Load dataset paths and labels from ucf101_noleakage\"\"\"\n",
    "        split_file = os.path.join(self.root_dir, f'{self.split}list01.txt')\n",
    "        try:\n",
    "            with open(split_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    video_path, label = line.strip().split()\n",
    "                    frame_path = os.path.join(self.frames_dir, video_path)\n",
    "                    flow_path = os.path.join(self.flows_dir, video_path.replace('.jpg', '.npy'))\n",
    "                    if os.path.exists(frame_path) and os.path.exists(flow_path):\n",
    "                        self.samples.append((frame_path, flow_path, int(label)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset from {split_file}: {e}\")\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path, flow_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load frame and flow\n",
    "        frame = load_video_frame(frame_path)\n",
    "        flow = load_optical_flow(flow_path)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "            # Only apply spatial transformations to flow, not color transformations\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                for t in self.transform.transforms:\n",
    "                    if isinstance(t, (transforms.RandomHorizontalFlip, transforms.RandomRotation)):\n",
    "                        flow = t(flow)\n",
    "            \n",
    "        return frame, flow, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train-val-test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (frames, flows, labels) in enumerate(dataloader):\n",
    "        frames, flows, labels = frames.to(device), flows.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames, flows)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 99:\n",
    "            print(f'Batch: {batch_idx+1}, Loss: {running_loss/100:.3f}, '\n",
    "                  f'Acc: {100.*correct/total:.2f}%')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return correct/total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, flows, labels in dataloader:\n",
    "            frames, flows, labels = frames.to(device), flows.to(device), labels.to(device)\n",
    "            outputs = model(frames, flows)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, accuracy, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_classes = 101  # UCF101 dataset\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = DualStreamNetwork(num_classes).to(device)\n",
    "\n",
    "# Setup data\n",
    "data_root = '/dtu/datasets1/02516'  # Adjust path as needed\n",
    "train_dataset = FlowVideoDataset(root_dir=data_root, split='train')\n",
    "val_dataset = FlowVideoDataset(root_dir=data_root, split='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                        shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_acc = 0.0\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    # Train\n",
    "    train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Training Accuracy: {train_acc*100:.2f}%')\n",
    "    \n",
    "    # Validate\n",
    "    val_acc, val_loss = validate(model, val_loader, criterion, device)\n",
    "    print(f'Validation Accuracy: {val_acc*100:.2f}%, Loss: {val_loss:.3f}')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        save_checkpoint(model, optimizer, epoch, val_acc,\n",
    "                        os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "    \n",
    "    # Save regular checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, val_acc,\n",
    "                        os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "    print(f'Best accuracy: {best_acc*100:.2f}%')\n",
    "    print('-' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
